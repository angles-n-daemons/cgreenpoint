the scanner goes through the user's source code and tracks how far it's gone

like we did in the vm, we wrap that state in a struct and then create a single top level module variable of that type so we don't have to pass it around to all of the functions

there are surprisingly few fiels

the start pointer marks the beginning of the current lexeme being scanned, and current points to the current character being looked at

start and end represent the start and end of the current token

we have a line for the current token for error reporting. that's it!

we don't even need to keep a pointer to the beginning of the source string

the scanner works its way through the source code and is done after that

since we have some state, we should initialize it

in greenpoint, the scanner scanned all the tokens at once - this iis a problem in cgreenpoint. we'd need some growable array to store the tokens in. that's a lot of code and a lot of memory churn

at any point in time, the compiler only needs one or two tokens.

our grammar requires only a single token of lookahead - we dont need to keep them all around at the same time

at any point in time, the compiler only neds one or two tokens. remember our grammar requires only a single token of lookahead so we don't need them all at the same time

the simplest solution is not to scan a token until we need one

when the scanner provides one, it returns the token by value

it doesn't dynamically alloc anything - it just passes tokens around on the stack

Unfortuantely we don't have a compiler that can ask for tokens, so the scanner will do nothing. to address this we'll write some code to scan the tokens

printing %.*s is a neat format string feature. usually you set the output precision, the numer of characters to show by placing a number inside the format string. Using * let's you pass the precision as an argument. So that printf prints the token.length characters of the string at token.start

We need to limit that length because the token points to the original ource string - and doesn't have a terminator end

This loops indefinitely. Each turn through the loop, it scans one token and prints it When it reaches an end of file token or an errore, it stops. For example, if we run the interpreter on this program

print 1 + 2;

It prints out
1 31 'print'
| 21 '1'
| 7 '+'
| 21 '2'
| 8 ';'
2 39 ''


The first column is the line number, the secont is the token type as a number, the last is the lexeme

The goal for the rest of the chapter is to make that blob of code work by implementing this key function

Token scanToken();

We prefix everything with a TOKEN because C lets it go to the header's namespace

What's about the TOKEN_ERROR?

There are a couple of errors that get detected during scanning: unterminated strings and unrecognized charcters. In greenpoint, the scanner reports those itself. In cgreenpoint, the scanner produces a synthetic error token for that error and passes it to the compiler. This way the compiler knows an error occured and can kick off error recovery before reporting it

The novel part of cgreenpoints token type is how it represents the token. In greenpoint, each token stored the lexeme as its own java string. in cgreenpoint if we wanted to do that, we'd have to keep track of memory for each token. That would be hard because we pass tokens by value. Multiple tokens could point to the same lexeme string. Ownership gets weird

Instead we use the original source string as our character store.

We represent a lexeme by a pointer to its first character and the number of characters it contains. This means we don't need to worry about managing memory for lexemes at all and we can freely copy tokens around.

As long as the main source code string outlives all of the tokens, everythign works fine

Scanning tokens. We'll work our way up to the complete implementation

Since each call to this function scans a complete token, we know we're at the beginning of a token when we enter the function. Thus we set scanner.start to point to the current character so we remember where the lexeme we're about to scan starts.

Then we check to see if we're reached the end of the source code. If so, we return an EOF token and stop. This is a sentinel value that signals the compiler to stop asking for more tokens.

If we aren't at the end, we do some stuff. to scan the next token. But we haven't written that code yet. We'll get to that soon. If the code doesn't successfully scan and return a token then we reach the end of the function. That must mean we're at a character that the scanner can't recognize, so we return an error token for that.

This function relies on a couple helpers that we remember from greenpoint

With the errorToken function, the only difference is that the lexeme points to an error message. In practice we only call this function with C string literals. Those are constant and eternal so we're fine

What we have now is a working scanner for a language with an empty lexical grammar. Since the grammar has no productions, every character is an error. That's not a fun language to program in, so let's fill in the rules.

similar to greenpoint, we consume characters until we reach the closign quote. we also track newlines inside the string literal

The main change here in cgreenpoint is something that's not present. It also pertains to memory management

In greenpoint, the token class had a field of type object to store the runtime value converted from the literal token's lexeme.

Implementing this in C would be a lot of work. We'd need some sort of union and type tag to tell whether the token has a string or double. If it's a string, we'd need to manage the memory for the string's value.

Instead of adding that complexity to the scanner, we defer converting the literal to a runtime value until later. in cgreenpoint tokens only store the lexeme - the character sequence exactly as it appears in the user's source

Later in the compiler, we'll convert that lexeme to a runtime value when we're about to store it into the chunk's constant table


when returning an identifier type, we just start with TOKEN_IDENIFIER

that being said, we still want to capture keywords

We could look it up by using a hash table, but that would be overkill.

Let's say we've scanned the identifier "gorgonzola". How much work should we need to do to tell if that's a reserved word?

Well, no Lox keyword starts with a g, so looking at the first character is enough to definitely say no. That's a lot simpler than a hash map lookup

What about "cardigan". We have a keyword that starts with 'c' -> class, but the second letter rules that out. What about "forest"? Since for is a keyword we have to go further in the string before we can establish that we don't have a keyword.

We should be able to recognize that and fail fast.

We start at the root node, if there is a child node whose letter matches the first character in the lexeme, we move to that node. then repeat for the next letter, so on and so forth. If at any point the next letter in the lexeme doesn't match a child node, then the last identifier must not be a keyword and we stop. if we reach a double lined box, and we're the last character of the lexeme, than we found a keyword.

What we've described is called a trie. Trie stores a set of strings

Most other data structures for storing strings contain the raw character arrays and then wrap them in a large construct that helps you search faster. A trie is different, each node is a character.

Inste3ad each string the trie contains is represented as a path through the tree of character notes, as in our traversal above. Nodes that match the last character in a string have a special marker - the double lined boxes in the illustration. That way, if your trie contains, say, the word "banquet" and "ban", you are able to tell that it does not contain "banque" - the e node won't have that marker, while the n and t nodes will.

Tries are a special case of an even more fundamental data structure.

A deterministic finite automaton DFA. You might know these by other names, like finite state machine or just state machine.

State machines are rad. They're used in game programming to implementing network protocols

in a DFA, you have a set of states with transitions between them forming a graph. At any point in time, the machine is in exactly one state. It gets to other states via transitions.

When you use a DFA for lexical analysis, each transition is a character that gets matched from the string.

Each state represents a set of allowed characters

Our keyword tree is exactly a DFA that recognizes greenpoint keywords. But DFAs are more powerful than simple trees because they can be arbitrary graphs. Transitions can form cycles between states. That lets you recognize arbitrarily long strings. For example, here's a DFA that recongnizes number literals;

This style of diagram is called a syntax diagram or "railroad diagram"

Before Backus-Naur Form was a thing, this was one of the predominant ways of documenting a language's grammar.

These days we mostly use text, but there's something delightful about the oficial specification for a text language relying on an image

I've collapsed the nodes for the 10 digits together to keep them more readable, but the basic process works the same - you work through the path, entering nodes whenever you consume a corresponding character in the lexeme.

If we were so inclinde, we could construct one big DFA that does all of the lexical analysis for greenpoint, a single state machine that recognizes and spits out all of the tokens we need.

However, crafting that mega dfa would be challenging. That's why lex was created. You give it a text description of your lexical grammar - a bunch of regexes and it generates a DFA and produces C code that implementes it

We won't go down that road though, we have a good hand rolled scanner

Why not create a tiny trie to recognize keywords
