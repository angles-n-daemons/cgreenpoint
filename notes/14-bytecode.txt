We already have ourselves a complete implementation of greenpoint, so why isn't the book over yet? It's because greenpoint relies on the JVM to do lots of things for us. If we want to underrstand how an interpreter works all the way to the metal, we need to build those bits and pieces ourselvess.

An even more fundamental reason that greenpoint isn't sufficient is because it's too slow. A tree-walk interpreter is fine for some kinds of high level declarative langauges. But for a general purpose, imperitive language - even scripting languages it won't fly

We could tune it all we want, but the design of walking the AST is fundamentally wrong. We can't micro optimize it to the point that we want.

Let's rethink our core model. This chapter introduces that model, bytecode and begins our new interpreter - cgreenpoint


## bytecode

In engineering, it's tough to go without trade offs. To understand why we're doing it - let's look at the alternatives

Walking the AST
benefits
- Well, we already have this. The other reason we don't want to do it is it's really simple to implement. The runtime representation of the code directly maps to the syntax. it's effortless to get from parser to the data structures we need at runtime
- it's portable. because it's on the JVM, it can run in any platform java supports. We could write a new c implementation but need to compile it to every system we need it to run on

These are the benefits, but it's not memory efficient. Each piece of syntax becomes an AST Node. A tiny lox expression like 1 + 2 turns into a whole heap of objects with various properties

Each pointer adds an extra 32 or 64 bites of overhead to the object. Worse, sprinkling our data across the heap in a loosely connected web of objects is bad for spatial locality

Modern CPUs process data way faster they they can pull them from RAM. To compensate, chips have multiple layers of caching. if a piece of memory is in the cache, it can be loaded up to 100x faster than from memory.

How does that data get into the cache however? The machine speculates stuff in there. It's heuristic is pretty simple. Whenever the CPU reads a bit of data from RAM, it pulls in a whole bundle of adjacent bytes and stuffs them in the cache.

If our prrogram next requests soe data close enough to be in that cache line, our CPU runs like a well oiled conveyor belt in a factorry. we really wanna take advantage of this.

To use the cache effectively, the way we represent code in memory should be dense and ordered like its read

Our AST walker has other overhead too around interface dispatch and visitor pattern, but the locality issues are enough to justify a better code representation

Compiling to Native Code??

If you want to go really fast, you want all of those layers of indirection out of the way. All the way down to the metal, machine code. It even sounds fast. Machine code

Compiling to the instruction set the chip supports is what the fastest langauges do. Targeting native code has always been the most efficient option.

If you've never written any machine code, or assembly code before - here's a gentle introduction.

Native code is a dense series of operations encoded directly in binary. Each instruction is between one and a few bytes long and is super low level. Add two numbers, move memorry from here to here

The CPU cranks through these instructions, decoding and executing each one in order. There is no tree structure like in our AST, and control flow is handled by jumping fro one point in the code to another. No indirection, no overhead, no skipping or chasing pointers.

It's fast, but comes at a cost. FIrst is that compiling to machine code is not easy. Modern instruction sets are super complex.

Second is it's not portable. If you wanna compile on a different machine, you need to write a new compilation back end

what is bytecode?

fix those two points in your mind. on one end, a tree walk interpreter is simple, portable and slow. native code is complex, platform specific but fast. bytecode sits in the middle. it retains the portability of a tree walker (we don't have to write assembly) but sacrifices some simplicity to get a performance boost.

Structurally, bytecode represents machine code. It's a dense linear sequence of binary instructions. that keeps overhead low and plays nice with the cache. that being said, it's a simpler, higher level instruction set than any real chip out there. In bytecode formats, instructions are usually just a byte long

Imagine you're writing a native compiler from some source language and you're given carte blance to define the easiest possible architecture to target. bytecode is kinda like that. It's a fantasy instruction set that makes your life as the compiler wirter easierr.

The problem with fantasies is that they don't exist. We solve this by writing an emulator - a simulated chip written in software that interprets the bytecode one instruction at a time. A virtual machine

The emulation layer adds overhead, which is why bytecode is slower - but adds portablility. Write our VM in a langauge like C that is already supported on all machines and we can run our emulator on top of any hardware we like.

this is what we'll do with cgreenpoint. we'll follow things like python, ruby, lua, ocaml, erlang, and others.

our vm will parrallel the structure of our previous design:

greenpoint                 cgreenpoint
parserr										 compiler
   v													v
syntax trees							 bytecode
   v													v
interpreter								virtual machine

Of course, we won't implmement the phases strictly in order. Like our previous interpreter, we'll bounce arround, building up one language feature at a time. in this chapter, we'll get the skeleton of the application in place and create the data structures needed to store and represent a chunk of bytecode.


We're going to create a chunk file, which has an enum to represent instructions and a chunk type. A chunk type is a wrapper around an array of data. Dynamic arrays are useful because they provide cache friendly, dense storage - constant time look up - constant time appending to the end of the array
These features are why dynamic arrays are used all over the place in cgreenpoint

Now that we're in c we get to roll our own.

Dynamic arrays are pretty simple, In addition to the array itself, we keep two numbers: the number of elements in the array we've allocated ("capacity") and how many are actually used ("count")

When we add an element, if the count is less than the capactiy, then there's space for it it in the array. We store an element and bump the count

If we have no capacity, we have to allocate new data, copy the data over to the new array and free the existing array before adding the new data

We have our struct ready, so let's implement the functions to work with it. C doesn't have constructors, so we declarre a function to init a new chunk

Copying the existing elements when you seems like it's O(n) (yah) not constant time. However if you continue to grow the capacity by the current size (ie double each time you need more space) - it averages out to constant time over time

The dynamic array starts off completely empty. We don't even allocate a raw array yet. To append a byte to the end of a chunk, we use a new function.

# disassembling chunks

now that we have a little module for creating chunks of bytecode

let's try it out

how do we know what's happening?

all we've done is push some bytes around in memory. we have no human friendly way to see what's actually inside that chunk we made

To fix this we're going to create a siassembler. An assembler is an old school program that takes a human rreadable mnemonic names for CPU instructions like ADD and MULT and translates them to their binary machine equivalent. a disassembler goes in the opposite direction - given a blob of achine code, it spits out a textual listing of instructions.

We'll implement something similar. Given a chunk - it will print out all the instructions on it. A lox user won't use this, but Lox maintainers will benefit because we get a window into the interpreters internal representation of code.

in main(), after we create the chunk we'll pass it to the disassembler

to disassemble a chunk, we print a little header (so we can tell which chunk we're looking at) and then crank through the bytecode, disassembling each instruction. the way we iterate through the code is a little odd. instead of incrementing i in the loop, we let disassembleinstruction do it for us. when we call that function, after disassembling the instruction at the given offset, it returns the offset at the next instruction. this is because we'll see that instructions have different sizes

The core of the debug module is this function:

first it prints the byte offset of the instruction (telling us where it is in the chunk) this is going to be a helpful signpost when we start doing control flow and jump around in the bytecode

next it reads a single byte from the bytecode at the given offset. that's our opcode. we switch on that. for each kind of instruction, we dispatch to a little utility function for displaying it. just in case the byte doesn't look like anything we know, we print that too (likely a bug)


There isn't much going to the returrn instruciton so all it does is print the op code and then rerturn the next byte offset. other instructions will have more going on


now that we have a rudimentary chunk structure working, let's start making it more useful. we can store code in chunks, but what about data? many values the interpreter works iwth are created at runtime as the result of operations:

1 + 2

The value 3 appears nowhere in the code. however the literals 1 and 2 do. To compile that statement to bytecode we need some sort of instruction that means "produce a constant" and those literal values need to get stored on the chunk soemwhere. in greenpoint, expr.literal ast node held the value. we need a different solution now that we don't have a syntax tree

we won't be running any code in this chapter, but since constants have a foot in both static and dynamic worlds of our interpreter, they force us to start thinking at least a little bit about how ourr VM should represent values.

For now we're going to start as simple as possible - we'll support only double-precision, floating point numbers. This will obviously expand over time, so we'll set up a new module to give ourselves room to grow

This typedef abstracts how greenpoint values are concretely represented in C. That way, we can change taht representation without needing to go back and fix existing code that passes around values.

back to the question where to store constants in a chunk. For small fixed-size values like integerrs, many instruction sets store the value directly in the codestream right after the opcode. these are called immediate instructions because the bits for the value are immediately after the opcode.

immediate instructions - data right after opcode

This doesn't work well for large or variable sized constants like strings. in a native compiler to machine code, those bigger constants get stored in a separate "contant data" reigion in the binary executable. then the instruction to load a constant has an address to where it's stored

Most virtual machines do something similar. For example - the JVM associates a constant pool with each compiled class. That sounds good enough for cgreenpoint to me. Each chunk will carry with  it a list of values that appear as literals in the programs. To keep things simpler, we'll put all constants in there, even simple integers


Value Arrays

The constant pool is an array of values. The isntruction to load a constant looks up the value by index in that array. As with our bytecode array - the compiler doesn't know how big the array needs to be ahead of time.


constant instructions

we can store constants in chunks, but we also need to execute them. in a piece of code like:

print 1;
print 2;

The compiled chunk needs to not only contain the values 1 and 2, but also know when to produce them so that they are printed in the right order. Thus we need an instruction that produces a particular constant

When the VM executes a constant, it loads the constant for use.

This new instruction is a little more complex than OP_RETURN. IN the above examplel, we load two different constants. A single barre opcode isn't enough to know which constant to load.

To handle cases like these, our bytecode - like most others - allows instructions to have operands. These are stored as binary data immediately after the opcode in the input stream and let us parameterize what the instruction does.

Each opcode determines how many operand bytes it has and what they mean. For example, a simple operation like "return" may have no operands, where an instruction for "load local variable" needs an operand to identify which variable to load. Each time we add a new oopcode to cgreenpoint, we specify what its operands look like - it's instruction format.

In this case, OP_CONSTANT takes a singel byte operand that specifies which constant to load from the chunk's constant array. Since we don't have a compiler yet, we hand compile an instruction in our test chunk.


# Line information

Chunks contain almost all of the information that the runtime needs from the user's source

It's kind of crrazy to think that we can rreduce all the different AST classes that we created in greenpoint down to an array of bytes and array of constants

There's on piece of data we're missing - we need it even though we hope the user neverr sees it

When runtime errors occur, we show the user the line number of the offending source code. In jlox, those numbers live in tokens which we in turn store in the AST nodes. We need a different solutionf for clox now that we've ditched syntax trees in favor of bytecode.

Given any bytecode instruction, we need to be able to determine the line of the user's source program that it was compiled from

There are lots of clever ways to encode this, I took the simplest approach I could come up with even though it's embarassingly inefficient with memory. In the chunk, we store a separate array of integers parallel to the bytecode. When runtime errors occur, we look up the line number at the same index.

There's a benefit to this, because the array is separate - the value is only pulled up if there's an error. This saves space in the compact encoding that out code takes

We add this array to chunk, and everywhere chunk is used

okay so this works

once we have a real front end the compiler will track the current line as it parses and pass that in

now that we have line information for every instruction, lets put it to good use

in our disassembler, it's helpful to show which source line each instruction was compiled from

Bytecode instructions tend to be pretty fine grained. A single line of source generally translates to a whole sequence of instructions. To make that clear, we show a | for instructions that occur on the same line as the previous one
the result output looks like:

== test chunk ==
0000  123 OP_CONSTANT         0 '1.2'
0002    | OP_RETURN


we have a three byte chunk. the first two bytes are a constant instruction that loads 1.2 from the chunks constant pool, the first byte is the op constant opcode and the second is the index in the constant pool.

the third byte is a single byte return instruction

In the remaining chapters, we'll flesh this out with lots more kinds of instructions. the basic structure is here though, everything we need to completely represent an executable piece of code at runtime in our virtual machine. remember the whole family of AST classes we defined in greenpoint? in cgreenpoint we've reduced that down to three arrays:

1. bytes of code,
2. constant values
3. line information for debugging

this reduction is a key reason why our new interpreter will be faster than greenpoint. you can think of bytecode as a compact serialization of the ast. highly optimized for how the interpreter will deserialize it in the order it needs as it executes

in the next chapter we'll see how the virutal machine does exactly that

Challenges:

1. Our encoding of line information is hilariously wasteful of memory. Given that a series of instructions often correspond to the same source line, a natural solution is something akin to run-length encoding of the lines

run-length encoding is a form of lossless data compression in which runs of data (sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count - rather than the original run. this is most efficient on data that contains many runs, for example simple graphic images such as icons, line drawings, etc.

For files that don't have many runs, RLE could increase file size

Devise an encoding that compresses the line information for a series of instructions on the same line. Change writeChunk to wrrite this compressed form, and implmement a getLIne function that given the index of an instruction determines the line where the instruction occurs.


----

because OP_CONSTANT uses a single byte for its operand, a chunk may only contain up to 256 constants. that's small enough that people writing real world code will hit that limit. we could use two or more bytes to store the operand, but that makes every constant instruction take up more space. most chunks wont need that many unique constants so that wastes space and sacrifices some locality in the common case to support the rare case.


To balance those two competing aims, many instruction sets feature multiple instructions that perform the same operation but with operands of different sizes.

Leave our existing one-byte OP_CONSTANT instruction alone, and define a second OP_CONSTANT_LONG instruction. It stores the operand as a 24 bit number which should be plenty.

Implement this function:

void writeConstant(Chunk* chunk, Value value, int line);

It adds value to the chunks constant array and then writes an appropriate instruction to load the constant.

Also add suport to the disassembler for OP_CONSTANT_LONG instructions.

Defining two instructions seems to be the best of both worlds. What sacrifices if any does it force on us?


Okay how should I have done the line changes?

I could have kept track of line counts
Disassembly becomes messy though in that case


how does realloc work?

- realloc(ptr, size)

   if pointer == null
      return malloc(size);

   size oldsize = malloc_getsize(ptr)

   // are we shrinking? thats easy
   if (size < oldsize) {
      malloc_setsize(ptr, size);
      return ptr;
   }

   //can we grow the allocation in place?
   if (malloc_can_grow(ptr, size)) {
      malloc_setsize(ptr, size)
      return ptr

   }

   void nptr = malloc(size)
   if newptr == NULL
      return NULL
   
   memcpy(newptr, ptr, oldsize);
   free(ptr);
   return newptr;
